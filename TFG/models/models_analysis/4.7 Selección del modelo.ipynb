{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 4.7 Selección del modelo\r\n",
    "## Configuración inicial\r\n",
    "\r\n",
    "En este documento se generan modelos LSTM con distintas entradas de tiempo y salidas.\r\n",
    "Las entradas de tiempo vienen predefinidas para 48, 36, 24, 12 y 6 horas.\r\n",
    "Las salidas deben indicarse en la variable OUT_STEPS"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Importando librerías y cargando los datos"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import IPython\r\n",
    "import IPython.display\r\n",
    "import matplotlib as mpl\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import tensorflow as tf\r\n",
    "\r\n",
    "mpl.rcParams['figure.figsize'] = (6, 4)\r\n",
    "mpl.rcParams['axes.grid'] = False\r\n",
    "\r\n",
    "from keras.datasets import imdb\r\n",
    "from keras import models, layers, optimizers\r\n",
    "\r\n",
    "import json, os"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "data_file = os.getcwd() + \"/in_models/historical_data_to_model_2.csv\"\r\n",
    "df = pd.read_csv(data_file, low_memory=False)\r\n",
    "label = 'AQI'\r\n",
    "df.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(17141, 21)"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Indicar número de OUT_STEPS (predicciones)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "OUT_STEPS = 3\r\n",
    "INPUT_WIDTH = 24 "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "df.columns"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['AQI', 'weekday', 'speed', 'travel_time', 'Minimum Temperature',\n",
       "       'Maximum Temperature', 'Temperature', 'Dew Point', 'Relative Humidity',\n",
       "       'Precipitation', 'Snow Depth', 'Visibility', 'Cloud Cover',\n",
       "       'Sea Level Pressure', 'Conditions', 'Wx', 'Wy', 'Day sin', 'Day cos',\n",
       "       'Year sin', 'Year cos'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### División de los datos\n",
    "Se hace una división (70%, 20%, 10%) para los conjuntos de entrenamiento, validación y prueba respectivamente."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "column_indices = {name: i for i, name in enumerate(df.columns)}\r\n",
    "\r\n",
    "n = len(df)\r\n",
    "train_df = df[0:int(n*0.7)]\r\n",
    "val_df = df[int(n*0.7):int(n*0.9)]\r\n",
    "test_df = df[int(n*0.9):]\r\n",
    "\r\n",
    "num_features = df.shape[1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "print(\"Train: \", train_df.shape)\r\n",
    "print(\"Validation: \", val_df.shape)\r\n",
    "print(\"Test: \", test_df.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train:  (11998, 21)\n",
      "Validation:  (3428, 21)\n",
      "Test:  (1715, 21)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Normalizar los datos\n",
    "Se guardan los datos de entreno en un CSV aparte, ya que estos son los datos que se utilizarán para normalizar cualquier dato que se le proporcione al modelo."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "train_path = os.getcwd().split(\"TFG\")[0] + \"TFG\\\\in\\\\normalize_data\\\\train_data_LSTM_models.csv\"\r\n",
    "train_df.to_csv(train_path, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "train_mean = train_df.mean()\r\n",
    "train_std = train_df.std()\r\n",
    "train_df = (train_df - train_mean) / train_std\r\n",
    "val_df = (val_df - train_mean) / train_std\r\n",
    "test_df = (test_df - train_mean) / train_std"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ventana de datos\n",
    "Los modelos de este estudio harán un conjunto de predicciones basadas en una ventana de muestras consecutivas de los datos.\n",
    "\n",
    "Esta sección se centra en implementar la ventana de datos para que pueda reutilizarse para todos los modelos.\n",
    "\n",
    "El resto de esta sección define una clase `WindowGenerator` . Esta clase permite:\n",
    "\n",
    " 1. Manejar los índices y compensaciones.\n",
    " 2. Dividir las ventanas de entidades en pares (features, labels).\n",
    " 3. Trazar el contenido de las ventanas resultantes.\n",
    " 4. Generar de manera eficiente lotes de estas ventanas a partir de los datos de entrenamiento, evaluación y prueba, utilizando `tf.data.Datasets`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Índices y compensaciones\n",
    "Se crea la clase `WindowGenerator`. El método `__init__` incluye toda la lógica necesaria para los índices de entrada y etiqueta.\n",
    "\n",
    "También toma los marcos de datos de entrenamiento, validación y prueba como entrada. Estos se convertirán a `tf.data.Datasets` de Windows más adelante."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "class WindowGenerator():\r\n",
    "  def __init__(self, input_width, label_width, shift,\r\n",
    "               train_df=train_df, val_df=val_df, test_df=test_df,\r\n",
    "               label_columns=None):\r\n",
    "    # Store the raw data.\r\n",
    "    self.train_df = train_df\r\n",
    "    self.val_df = val_df\r\n",
    "    self.test_df = test_df\r\n",
    "\r\n",
    "    # Work out the label column indices.\r\n",
    "    self.label_columns = label_columns\r\n",
    "    if label_columns is not None:\r\n",
    "        self.label_columns_indices = {name: i for i, name in\r\n",
    "                                    enumerate(label_columns)}\r\n",
    "    self.column_indices = {name: i for i, name in\r\n",
    "                           enumerate(train_df.columns)}\r\n",
    "\r\n",
    "    # Work out the window parameters.\r\n",
    "    self.input_width = input_width\r\n",
    "    self.label_width = label_width\r\n",
    "    self.shift = shift\r\n",
    "\r\n",
    "    self.total_window_size = input_width + shift\r\n",
    "\r\n",
    "    self.input_slice = slice(0, input_width)\r\n",
    "    self.input_indices = np.arange(self.total_window_size)[self.input_slice]\r\n",
    "\r\n",
    "    self.label_start = self.total_window_size - self.label_width\r\n",
    "    self.labels_slice = slice(self.label_start, None)\r\n",
    "    self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\r\n",
    "\r\n",
    "  def __repr__(self):\r\n",
    "    return '\\n'.join([\r\n",
    "        f'Total window size: {self.total_window_size}',\r\n",
    "        f'Input indices: {self.input_indices}',\r\n",
    "        f'Label indices: {self.label_indices}',\r\n",
    "        f'Label column name(s): {self.label_columns}'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Split / Dividir\n",
    "Dada una lista de entradas consecutivas, el método `split_window` la convertirá en una ventana de entradas y una ventana de etiquetas."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def split_window(self, features):\r\n",
    "    inputs = features[:, self.input_slice, :]\r\n",
    "    labels = features[:, self.labels_slice, :]\r\n",
    "    if self.label_columns is not None:\r\n",
    "        labels = tf.stack(\r\n",
    "            [labels[:, :, self.column_indices[name]] for name in self.label_columns],\r\n",
    "            axis=-1)\r\n",
    "\r\n",
    "    # Slicing doesn't preserve static shape information, so set the shapes\r\n",
    "    # manually. This way the `tf.data.Datasets` are easier to inspect.\r\n",
    "    inputs.set_shape([None, self.input_width, None])\r\n",
    "    labels.set_shape([None, self.label_width, None])\r\n",
    "\r\n",
    "    return inputs, labels\r\n",
    "\r\n",
    "WindowGenerator.split_window = split_window"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Plot\n",
    "Método de trazado que permite una visualización simple de la ventana dividida:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def plot(self, model=None, plot_col=label, max_subplots=3):\r\n",
    "    inputs, labels = self.example\r\n",
    "    plt.figure(figsize=(10, 6))\r\n",
    "    plot_col_index = self.column_indices[plot_col]\r\n",
    "    max_n = min(max_subplots, len(inputs))\r\n",
    "    for n in range(max_n):\r\n",
    "        plt.subplot(max_n, 1, n+1)\r\n",
    "        plt.ylabel(f'{plot_col} [normed]')\r\n",
    "        plt.plot(self.input_indices, inputs[n, :, plot_col_index],\r\n",
    "                 label='Inputs', marker='.', zorder=-10)\r\n",
    "\r\n",
    "        if self.label_columns:\r\n",
    "            label_col_index = self.label_columns_indices.get(plot_col, None)\r\n",
    "        else:\r\n",
    "            label_col_index = plot_col_index\r\n",
    "\r\n",
    "        if label_col_index is None:\r\n",
    "            continue\r\n",
    "\r\n",
    "        plt.scatter(self.label_indices, labels[n, :, label_col_index],\r\n",
    "                    edgecolors='k', label='Labels', c='#2ca02c', s=64)\r\n",
    "        if model is not None:\r\n",
    "            predictions = model(inputs)\r\n",
    "            plt.scatter(self.label_indices, predictions[n, :, label_col_index],\r\n",
    "                        marker='X', edgecolors='k', label='Predictions',\r\n",
    "                        c='#ff7f0e', s=64)\r\n",
    "        if n == 0:\r\n",
    "            plt.legend()\r\n",
    "            \r\n",
    "    plt.xlabel('Time [h]')\r\n",
    "    plt.show()\r\n",
    "    \r\n",
    "\r\n",
    "WindowGenerator.plot = plot"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " El método `plot loss` sirve para graficar los valores de perdida en la fase de entreno y validación para ayudar a identificar si hay overfitting o underfitting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def plot_loss(history):\r\n",
    "    history_dict = history.history\r\n",
    "    loss_values = history_dict['loss']\r\n",
    "    val_loss_values = history_dict['val_loss']\r\n",
    "\r\n",
    "    fig = plt.figure(figsize=(5,5))\r\n",
    "    epoch = range(1,len(loss_values)+1)\r\n",
    "    plt.plot(epoch,loss_values, 'o',label='training')\r\n",
    "    plt.plot(epoch,val_loss_values, '--',label='val')\r\n",
    "    plt.xlabel(\"epoch\")\r\n",
    "    plt.ylabel(\"loss\")\r\n",
    "    plt.legend()\r\n",
    "    plt.show()\r\n",
    "    \r\n",
    "WindowGenerator.plot_loss = plot_loss"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. tf.data.Datasets\n",
    "Finalmente, el método `make_dataset` tomará una serie de tiempo `DataFrame`  y lo convertirá en un `tf.data.Dataset` de `(input_window, label_window)` usando la función `preprocessing.timeseries_dataset_from_array`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def make_dataset(self, data):\r\n",
    "    data = np.array(data, dtype=np.float32)\r\n",
    "    ds = tf.keras.preprocessing.timeseries_dataset_from_array(\r\n",
    "      data=data,\r\n",
    "      targets=None,\r\n",
    "      sequence_length=self.total_window_size,\r\n",
    "      sequence_stride=1,\r\n",
    "      shuffle=True,\r\n",
    "      batch_size=32,\r\n",
    "      )\r\n",
    "    ds = ds.map(self.split_window)\r\n",
    "\r\n",
    "    return ds\r\n",
    "\r\n",
    "WindowGenerator.make_dataset = make_dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "El objeto `WindowGenerator` contiene datos de entrenamiento, validación y prueba. Se agregan propiedades para acceder a ellas como `tf.data.Datasets` usando el método `make_dataset` anterior."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "@property\r\n",
    "def train(self):\r\n",
    "  return self.make_dataset(self.train_df)\r\n",
    "\r\n",
    "@property\r\n",
    "def val(self):\r\n",
    "  return self.make_dataset(self.val_df)\r\n",
    "\r\n",
    "@property\r\n",
    "def test(self):\r\n",
    "  return self.make_dataset(self.test_df)\r\n",
    "\r\n",
    "@property\r\n",
    "def example(self):\r\n",
    "  \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\r\n",
    "  result = getattr(self, '_example', None)\r\n",
    "  if result is None:\r\n",
    "    # No example batch was found, so get one from the `.train` dataset\r\n",
    "    result = next(iter(self.train))\r\n",
    "    # And cache it for next time\r\n",
    "    self._example = result\r\n",
    "  return result\r\n",
    "\r\n",
    "WindowGenerator.train = train\r\n",
    "WindowGenerator.val = val\r\n",
    "WindowGenerator.test = test\r\n",
    "WindowGenerator.example = example"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5. Estadísticas\n",
    "El método `model_statistics` imprime en pantalla infromación relevante del modelo. \n",
    "Los diccionarios declarados se utilizan para guardar los resultados de las pruebas para cada modelo."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "multi_val_performance = {}\r\n",
    "multi_test_performance = {}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def model_statistics(model, history, name):  \r\n",
    "    print(\"General statistics of the generated model:\")\r\n",
    "    multi_val_performance[f'{name} {multi_window.input_width}h'] = model.evaluate(multi_window.val)\r\n",
    "    print(\"Statistics of the model with the test data:\")\r\n",
    "    multi_test_performance[f'{name} {multi_window.input_width}h'] = model.evaluate(multi_window.test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creación de los modelos\n",
    "\n",
    "Para estos modelos, los datos de entrenamiento constan de muestras por hora. Aquí, los modelos aprenderán a predecir  3 horas del futuro, dadas 24 horas del pasado (indicadas en la variable `input_width`).\n",
    "\n",
    "Se genera el objeto Window que creará los cortes a partir del conjunto de datos, con los que se entrenaran los modelos en cuestión:"
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "multi_window = WindowGenerator(input_width=INPUT_WIDTH,\r\n",
    "                               label_width=OUT_STEPS,\r\n",
    "                               shift=OUT_STEPS)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Se crea el método `compile_and_fit` que se utilizará para compilar y entrenar los modelos que se irán proponiendo a lo largo de este documento."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "MAX_EPOCHS = 20\r\n",
    "\r\n",
    "def compile_and_fit(model, window, patience=2):\r\n",
    "    # The patience parameter is the amount of epochs to check for improvement\r\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\r\n",
    "                                                    patience=patience,\r\n",
    "                                                    mode='min')\r\n",
    "\r\n",
    "    model.compile(loss=tf.losses.MeanAbsoluteError(),\r\n",
    "                optimizer=tf.optimizers.Adam(),\r\n",
    "                metrics=[tf.metrics.MeanAbsoluteError()])\r\n",
    "\r\n",
    "    history = model.fit(window.train, epochs=MAX_EPOCHS,\r\n",
    "                      validation_data=window.val,\r\n",
    "                      callbacks=[early_stopping])\r\n",
    "\r\n",
    "    return history"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5. Modelo RNN\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "multi_window_48 = WindowGenerator(input_width=48,\r\n",
    "                               label_width=OUT_STEPS,\r\n",
    "                               shift=OUT_STEPS)\r\n",
    "multi_lstm_model_48 = tf.keras.Sequential([\r\n",
    "    # Shape [batch, time, features] => [batch, lstm_units]\r\n",
    "    # Adding more `lstm_units` just overfits more quickly.\r\n",
    "    tf.keras.layers.LSTM(32, return_sequences=False),\r\n",
    "    # Shape => [batch, out_steps*features]\r\n",
    "    tf.keras.layers.Dense(OUT_STEPS*num_features,\r\n",
    "                          kernel_initializer=tf.initializers.zeros()),\r\n",
    "    # Shape => [batch, out_steps, features]\r\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, num_features])\r\n",
    "])\r\n",
    "\r\n",
    "history = compile_and_fit(multi_lstm_model_48, multi_window_48)\r\n",
    "IPython.display.clear_output()\r\n",
    "print(\"General statistics of the generated model:\")\r\n",
    "multi_val_performance[f'LSTM input=48h out={OUT_STEPS}h'] = multi_lstm_model_48.evaluate(multi_window_48.val)\r\n",
    "print(\"Statistics of the model with the test data:\")\r\n",
    "multi_test_performance[f'LSTM input=48h out={OUT_STEPS}h'] = multi_lstm_model_48.evaluate(multi_window_48.test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "General statistics of the generated model:\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 0.1915 - mean_absolute_error: 0.1915\n",
      "Statistics of the model with the test data:\n",
      "53/53 [==============================] - 0s 3ms/step - loss: 0.2260 - mean_absolute_error: 0.2260\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "multi_window_36 = WindowGenerator(input_width=36,\r\n",
    "                               label_width=OUT_STEPS,\r\n",
    "                               shift=OUT_STEPS)\r\n",
    "multi_lstm_model_36 = tf.keras.Sequential([\r\n",
    "    # Shape [batch, time, features] => [batch, lstm_units]\r\n",
    "    # Adding more `lstm_units` just overfits more quickly.\r\n",
    "    tf.keras.layers.LSTM(32, return_sequences=False),\r\n",
    "    # Shape => [batch, out_steps*features]\r\n",
    "    tf.keras.layers.Dense(OUT_STEPS*num_features,\r\n",
    "                          kernel_initializer=tf.initializers.zeros()),\r\n",
    "    # Shape => [batch, out_steps, features]\r\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, num_features])\r\n",
    "])\r\n",
    "\r\n",
    "history = compile_and_fit(multi_lstm_model_36, multi_window_36)\r\n",
    "IPython.display.clear_output()\r\n",
    "print(\"General statistics of the generated model:\")\r\n",
    "multi_val_performance[f'LSTM input=36h out={OUT_STEPS}h'] = multi_lstm_model_36.evaluate(multi_window_36.val)\r\n",
    "print(\"Statistics of the model with the test data:\")\r\n",
    "multi_test_performance[f'LSTM input=36h out={OUT_STEPS}h'] = multi_lstm_model_36.evaluate(multi_window_36.test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "General statistics of the generated model:\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 0.1910 - mean_absolute_error: 0.1910\n",
      "Statistics of the model with the test data:\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.2253 - mean_absolute_error: 0.2253\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "multi_window_24 = WindowGenerator(input_width=24,\r\n",
    "                               label_width=OUT_STEPS,\r\n",
    "                               shift=OUT_STEPS)\r\n",
    "multi_lstm_model_24 = tf.keras.Sequential([\r\n",
    "    # Shape [batch, time, features] => [batch, lstm_units]\r\n",
    "    # Adding more `lstm_units` just overfits more quickly.\r\n",
    "    tf.keras.layers.LSTM(32, return_sequences=False),\r\n",
    "    # Shape => [batch, out_steps*features]\r\n",
    "    tf.keras.layers.Dense(OUT_STEPS*num_features,\r\n",
    "                          kernel_initializer=tf.initializers.zeros()),\r\n",
    "    # Shape => [batch, out_steps, features]\r\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, num_features])\r\n",
    "])\r\n",
    "\r\n",
    "history = compile_and_fit(multi_lstm_model_24, multi_window_24)\r\n",
    "IPython.display.clear_output()\r\n",
    "print(\"General statistics of the generated model:\")\r\n",
    "multi_val_performance[f'LSTM input=24h out={OUT_STEPS}h'] = multi_lstm_model_24.evaluate(multi_window_24.val)\r\n",
    "print(\"Statistics of the model with the test data:\")\r\n",
    "multi_test_performance[f'LSTM input=24h out={OUT_STEPS}h'] = multi_lstm_model_24.evaluate(multi_window_24.test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "General statistics of the generated model:\n",
      "107/107 [==============================] - 0s 2ms/step - loss: 0.1909 - mean_absolute_error: 0.1909\n",
      "Statistics of the model with the test data:\n",
      "53/53 [==============================] - 0s 2ms/step - loss: 0.2230 - mean_absolute_error: 0.2230\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "multi_window_12 = WindowGenerator(input_width=12,\r\n",
    "                               label_width=OUT_STEPS,\r\n",
    "                               shift=OUT_STEPS)\r\n",
    "multi_lstm_model_12 = tf.keras.Sequential([\r\n",
    "    # Shape [batch, time, features] => [batch, lstm_units]\r\n",
    "    # Adding more `lstm_units` just overfits more quickly.\r\n",
    "    tf.keras.layers.LSTM(32, return_sequences=False),\r\n",
    "    # Shape => [batch, out_steps*features]\r\n",
    "    tf.keras.layers.Dense(OUT_STEPS*num_features,\r\n",
    "                          kernel_initializer=tf.initializers.zeros()),\r\n",
    "    # Shape => [batch, out_steps, features]\r\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, num_features])\r\n",
    "])\r\n",
    "\r\n",
    "history = compile_and_fit(multi_lstm_model_12, multi_window_12)\r\n",
    "IPython.display.clear_output()\r\n",
    "print(\"General statistics of the generated model:\")\r\n",
    "multi_val_performance[f'LSTM input=12h out={OUT_STEPS}h'] = multi_lstm_model_12.evaluate(multi_window_12.val)\r\n",
    "print(\"Statistics of the model with the test data:\")\r\n",
    "multi_test_performance[f'LSTM input=12h out={OUT_STEPS}h'] = multi_lstm_model_12.evaluate(multi_window_12.test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "General statistics of the generated model:\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 0.1910 - mean_absolute_error: 0.1910\n",
      "Statistics of the model with the test data:\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2199 - mean_absolute_error: 0.2199\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "multi_window_6 = WindowGenerator(input_width=6,\r\n",
    "                               label_width=OUT_STEPS,\r\n",
    "                               shift=OUT_STEPS)\r\n",
    "multi_lstm_model_6 = tf.keras.Sequential([\r\n",
    "    # Shape [batch, time, features] => [batch, lstm_units]\r\n",
    "    # Adding more `lstm_units` just overfits more quickly.\r\n",
    "    tf.keras.layers.LSTM(32, return_sequences=False),\r\n",
    "    # Shape => [batch, out_steps*features]\r\n",
    "    tf.keras.layers.Dense(OUT_STEPS*num_features,\r\n",
    "                          kernel_initializer=tf.initializers.zeros()),\r\n",
    "    # Shape => [batch, out_steps, features]\r\n",
    "    tf.keras.layers.Reshape([OUT_STEPS, num_features])\r\n",
    "])\r\n",
    "\r\n",
    "history = compile_and_fit(multi_lstm_model_6, multi_window_6)\r\n",
    "IPython.display.clear_output()\r\n",
    "print(\"General statistics of the generated model:\")\r\n",
    "multi_val_performance[f'LSTM input=6h out={OUT_STEPS}h'] = multi_lstm_model_6.evaluate(multi_window_6.val)\r\n",
    "print(\"Statistics of the model with the test data:\")\r\n",
    "multi_test_performance[f'LSTM input=6h out={OUT_STEPS}h'] = multi_lstm_model_6.evaluate(multi_window_6.test)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "General statistics of the generated model:\n",
      "107/107 [==============================] - 0s 1ms/step - loss: 0.1894 - mean_absolute_error: 0.1894\n",
      "Statistics of the model with the test data:\n",
      "54/54 [==============================] - 0s 1ms/step - loss: 0.2239 - mean_absolute_error: 0.2239\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "model_path = os.getcwd().split(\"models_analysis\")[0] + \"all_models\\\\new_models\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "multi_lstm_model_48.save(f'{model_path}\\\\new_LSTM/multi_lstm_model_input_48_out_{OUT_STEPS}.h5')\r\n",
    "multi_lstm_model_36.save(f'{model_path}\\\\new_LSTM/multi_lstm_model_input_36_out_{OUT_STEPS}.h5')\r\n",
    "multi_lstm_model_24.save(f'{model_path}\\\\new_LSTM/multi_lstm_model_input_24_out_{OUT_STEPS}.h5')\r\n",
    "multi_lstm_model_12.save(f'{model_path}\\\\new_LSTM/multi_lstm_model_input_12_out_{OUT_STEPS}.h5')\r\n",
    "multi_lstm_model_6.save(f'{model_path}\\\\new_LSTM/multi_lstm_model_input_6_out_{OUT_STEPS}.h5')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "multi_performance = {'Validation': multi_val_performance, 'Test': multi_test_performance}\r\n",
    "tf = open(f\"{model_path}/statistics/LSTM/statistics_out_steps_{OUT_STEPS}.json\", \"w\")\r\n",
    "json.dump(multi_performance,tf)\r\n",
    "tf.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Referencias: Apache License, Version 2.0 . (17 de 06 de 2021). Time series forecasting  |  TensorFlow Core. Obtenido de TensorFlow: https://www.tensorflow.org/tutorials/structured_data/time_series"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.7 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "interpreter": {
   "hash": "cb266bb19b0d3845cfd44a8e13f5fd510eef317810cd30fbde3182944861c85f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}